---
title: "Introduction to $\bf{MultivarTV}"
author: "Brayan Ortiz"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    fig_caption: yes
    toc: true
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\VignetteKeywords{R, C++, Armadillo, total variation, thin plate splines, denoising, piece-wise constant signals, signal processing}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


```{r libsetup, include = FALSE, echo=FALSE}
library(plot3D)
library(fields)
library(MultivarTV)
# Towers function
towers <- function(x1,x2){
	n=length(x1)
	y = numeric(n)
	for (i in 1:n){ 
		y[i] = 1*(x1[i]>0.8)*1*(x2[i]>0.8)+0.5*1*(x1[i]>0.8)*1*(x2[i]<0.2)+1*1*(x1[i]<0.2)*1*(x2[i]<0.2)+0.5*1*(x1[i]<0.2)*1*(x2[i]>0.8)
	}
	return(y)
}

pcf <- function(x){
  if (x<0.1) return(0.5)
  else if (x>= 0.1 & x<0.6) return(3)
  else if (x>= 0.6 & x<0.8) return(0.5)
  else return(2)
}
pcwise <- function(vecx){
  y <- numeric(length(vecx))
  for (i in 1:length(vecx)){
    y[i] <- pcf(vecx[i])
  }
  return(y)
}
```

## Abstract

Total variation is a widely applicable regularization problem that is commonly used in signal and image denoising. We introduce the $\tt{R}$ package $\bf{MultivarTV}$, which is used for solving the multivariate total variation problem. Using an alternating direction method of multipliers algorithm (ADMM), we are able to solve problems large in sample size and number of features. 

## Introduction

To introduce total variation, we consider the univariate or one-dimensional case first. We begin by observing 
\[
y_i=f(x_i)+\epsilon_i,
\]
where $\mathbb{E}[\epsilon_i]=0$, $\mathbb{var}[\epsilon_i]=\sigma^2<\infty$ and $f\in \mathcal{F}$ (for $i=1,\ldots,N$). A total variation problem seeks to estimate $f$ by $\hat{f}$, such that
\[
\hat{f} = \min_{f\in\mathcal{F}} \frac{1}{N}\sum_{i=1}^N \left(y_i-f(x_i)\right)^2+\lambda P(f),
\]
where $P(f)=\int|f^{(1)}(t)|\partial t$ (i.e. the total variation norm). $\bf{MultivarTV}$ in the univariate setting uses a mesh based solution, which is described by Ortiz and Simon (2018). For a mesh based solution, we consider a set of $m$ points, $d_1,\ldots,d_m$ that need not have been observed and aim to estimate their value $\theta_j = f(d_j)$. Let $\theta_{(i)}= \theta_j$ such that $d_j\le x_i<d_{j+1}$ (nearest neighbor). The mesh based solution to the univariate problem approximates $f(x_1),\ldots,f(x_N)$ by the values $\hat{\theta}=(\hat{\theta}_{(1)},\ldots,\hat{\theta}_{(N)})$:
\[
\hat{\theta} = \min_{\theta\in \mathbb{R}^m} \frac{1}{N}\sum_{i=1}^N \left(y_i-\theta_{(i)}\right) + \lambda \sum_{j=1}^m \left|\theta_{j+1}-\theta_j\right|,
\]
where $\|D_m\theta\|_1 = \sum_{j=1}^m \left|\theta_{j+1}-\theta_j\right|$ using the $\ell_1$-norm and first difference matrix 
\[
D_m = \begin{bmatrix}
  -1 & 1 & 0 & \ldots & 0 & 0 \\
  0 & -1 & 1 & \ldots & 0 & 0 \\
  \vdots  & \vdots & \vdots& \ddots &  \vdots &  \vdots \\
    0 & 0 & 0 & ... & -1 & 1 
    \end{bmatrix}\in \mathbb{R}^{m-1\times m}.
\]
Ortiz and Simon (2018) show the objective is quickly solved using an ADMM algorithm for $m\ge\sqrt{N}$ (with regular spacing), which is enough for statistical optimality of the solution $\hat{\theta}$. 

Since the optimization problem encourages adjacent points $\theta$ to be the same, we draw piecewise-constant fits as our solution. To begin our demonstration, we generate some data from a piecewise constant function:

```{r datagen}
set.seed(123)
N <- 100
x <- matrix(runif(N),ncol=1)
y <- matrix(pcwise(x) + rnorm(N,0,0.1),ncol=1)
```

To fit a total variation solution to this data, we need only use the $\tt{mvtv}$ function, which by default will perform 5-fold validation and stores the solution path for each $\lambda$. Let $m = 20$ for this problem:

```{r mvtv1, echo=FALSE}
set.seed(123) # Setting a seed for cross-validation procedure
m <- matrix(20,ncol=1)
fit <- mvtv(x,y,m,verbose=FALSE) # Verbose = FALSE to suppress algorithm details
```

We include an S3 generic $\tt{plot}$ function with additional flags. By default, we plot the predicted surface for the model with minimum cross-validated mean squared error and overlay observed data:

```{r fig1, fig.cap = "Figure 1. Total variation solution for piecewise constant function at best cross-validated model. ", fig.width=3.5}
plot(fit)
```

The $\tt{plot}$ function can also be used to see fits at other values of $\lambda$, such as lambda corresponding to 1 standard error rule or an arbitrary number:

```{r fig2, fig.cap = "Figure 2. Total variation solution for piecewise constant function at other solutions.", fig.width=7}
plot(fit, lambda = fit$lambdas[fit$lambda_1se_ind])
title(main = "One standard error rule")
plot(fit, lambda = 2.0)
title(main = "Arbitrary value: 2.0")
```

Aside from the number of knots, the user can also define the mesh itself. Note that by setting $m=N$ and $d_1=x_1,\ldots,d_N=x_N$, we return the fused lasso solution (tibs): 

```{r fig3, fig.cap = "Figure 3. (a) Fused lasso given by MultivarTV. (b) Fused lasso given by genlasso.", message=FALSE, fig.width=7}
par(mfrow=c(1,2))
set.seed(123) # Setting a seed for cross-validation procedure
m <- matrix(N,ncol=1)
xsorted <- sort(x, index.return=TRUE)
mesh <- matrix(xsorted$x, ncol = 1)
ysorted <- matrix(y[xsorted$ix], ncol = 1)
fl_mvtv <- mvtv(mesh,ysorted,m,mesh = mesh, verbose=FALSE) # Verbose = FALSE to suppress algorithm details
plot(fl_mvtv)
title(sub = "(a)")

library(genlasso)
fl_genlasso <- fusedlasso1d(ysorted)
cvfl <- cv.trendfilter(fl_genlasso,verbose = FALSE)
plot(fl_genlasso,lambda=cvfl$lambda.min,xlab = "x",ylab="y",pch=20,cex=0.5)
title(sub = "(b)")
```


## $\bf{MultivarTV}$ for Multivariate Data

For multivariate data, the total variation problem becomes difficult, which can be explained by seeing what changes in the bivariate setting. A total variation problem seeks to constrain the amount a function changes or varies in it's first derivative, which in the univariate setting means setting
\[
P(f) = \int |f^{(1)}(x)|\partial x < M,
\]
which we approximate using first differences as $\sum_{j=1}^m \left|\theta_{j+1}-\theta_j\right|$ in our problem. In the bivariate setting, a function $f(x_1, x_2)$ has three first derivatives: $\frac{\partial}{\partial x_1}$, $\frac{\partial}{\partial x_2}$, and $\frac{\partial}{\partial x_1\partial x_2}$. Hence, our penalty parameter becomes
\[
P(f) = \int \left|\frac{\partial}{\partial x_1}  f(x_1, x_2) \right|\partial x_1\partial x_2 +\int \left|\frac{\partial}{\partial x_2}  f(x_1, x_2) \right|\partial x_1\partial x_2+\int \left|\frac{\partial}{\partial x_1 \partial x_2}  f(x_1, x_2) \right|\partial x_1\partial x_2.
\]
$\bf{MultivarTV}$ continues to approximate $P(f)$ using differences across a mesh. However, a bivariate mesh is a lattice: for $x_{1,1},\ldots,x_{1,N}$, we create a mesh $d_{1,1},\ldots,d_{1,m_1}$ that contains all of $x_1$ (and do the same for $x_2$). We merge $d_1$ and $d_2$ into a lattice such that 
\[
\theta = \begin{bmatrix}
  f(d_{11},d_{21}) & f(d_{12},d_{21})  & \ldots  & f(d_{1m_1},d_{21}) \\
  f(d_{11},d_{22}) & f(d_{21},d_{22}) & \ldots & f(d_{1m_1},d_{22})  \\
   \vdots &  \ddots &  \vdots &  \vdots \\
     f(d_{11},d_{2m_2}) & f(d_{12},d_{2m_2}) & ... &  f(d_{1m_1},d_{2m_2}) 
    \end{bmatrix} \in \mathbb{R}^{m_1\times m_2}.
\]
As before, $\theta_{(i)}$ defines the value of the function on the lattice point nearest a point $x_i$. Using the first difference matrix defined in the introduction, a mesh based solution to the bivariate problem roughly solves the following minimization problem:
\[
\hat{\theta} = \min_{\theta\in \mathbb{R}^{m_1\times m_2}} \frac{1}{N}\sum_{i=1}^N \left(y_i-\theta_{(i)}\right)^2 + \lambda \left( \|D_{m_1}\theta\|_1+\|D_{m_2}\theta^\top\|_1+\|D_{m_1}\theta D_{m_2}^\top \|_1 \right).
\]
If we flatten the matrix $\theta\in \mathbb{R}^{m_1\times m_2}$ into a vector $\theta^*\in \mathbb{R}^{m_1m_2}$ and carefully construct a difference matrix $D^*$, we can rewrite the minimization as
\[
\hat{\theta^*} = \min_{\theta^*\in \mathbb{R}^{m_1 m_2}} \frac{1}{N}\sum_{i=1}^N \left(y_i-\theta^*_{(i)}\right)^2 + \lambda \|D^*\theta^*\|_1.
\]
The same can be done for $p>2$. For details, see Ortiz and Simon (2018). We can fit the multivariate problem using the same ADMM algorithm as in the univariate case, so $\tt{mvtv}$ uses the same algorithm regardless of dimensionality. 

### Towers Function

We want to fit a multivariate surface as the solution to a total variation problem. One surface that we can use for demonstration is one that resembles four towers on a plain. In Figure 1, we plot the towers as a mesh and overlay points generated from the towers function with noise. 


```{r fig4, echo=FALSE, fig.cap = "Figure 4. Plot of N=100 points drawn from Towers function (drawn as mesh) with noise.", fig.width=4, fig.height = 4}
x1 <- seq(0,1,length.out=40); x2 <- x1
x1x2 <- expand.grid(x1,x2)
y <- matrix(towers(x1x2[,1],x1x2[,2]),nrow=40,ncol = 40)

set.seed(117)
z1 <- runif(100)
z2 <- runif(100)
z3 <- towers(z1,z2)
ynoisy <- z3 + rnorm(length(z3),0,0.5)

scatter3D(z1,z2,ynoisy,theta=30,phi=30,xlab="x1",ylab="x2",zlab="y",cex=1,pch=20,
          surf= list(x = x1,y = x2,z = y,facets=NA,fit = z3))
```


The noisy towers can be fit well by a total variation solution. In this example, we will also compare the total variation fit against a thin plate spline, the generalization of smoothing splines. Based on Figure 5, we indeed get four towers from our total variation solution. In Figure 6, we see the thin plate spline solution, which is much more smooth and provides hills instead of towers. 

```{r figure5, echo=TRUE, fig.cap="Figure 5. Total variation solution for noisy sombrero." , fig.width=4, fig.height = 4}
set.seed(117)
mym <- matrix(rep(sqrt(length(ynoisy)),2),ncol=1)
data <- cbind(z1,z2)
tvmod <- mvtv(data,ynoisy,mym,verbose=FALSE)
plot(tvmod,adddata = TRUE)
```


```{r figure6,echo=TRUE, fig.cap="Figure 6. Thin plate spline solution for noisy sombrero." , fig.width=4, fig.height = 4}
fit <- Tps(data,ynoisy)
out.p <- predictSurface(fit, xy = c(1,2))
plot.surface(out.p, type="p")
```

### Beyond Bivariate: Limitations

The primary implementation function $\tt{mvtv}$ accepts any number of features. However, there are some limitations. First, the generic $\tt{plot}$ function does not accept $\tt{mvtv}$-objects built on $p>2$. However, we can still plot the residuals vs fitted values for any size problem using the $\tt{plotResiduals}$ function:

```{r figure7, echo=TRUE, fig.cap= "Figure 7. Residuals plot for perfectly noisy data.", fig.width=3.5}
set.seed(117)
n <- 300
p <- 3
m <- 4
data <- matrix(runif(n*p),ncol = p)
y <- matrix(runif(n), ncol=1)
m <- matrix(rep(m,p))
set.seed(123) # cvfold will yield different stuff without seed
tv3 <- mvtv(data,y,m,verbose = FALSE) 

plotResiduals(tv3)
```

For uniform noisy data regressed on uniform noisy data, we expect a flat line fit to the mean, which is what we see (with some small wiggling). 

Another limitation is scaling: the number of optimization parameters is $m^p$, which means for modest $m$ and large $p$ the difference matrices (even though they are stored as sparse matrices) become larger than most personal laptops can store. At experimentation stages, it may be best practice to let $m=N^{\frac{1}{4}}$.  
